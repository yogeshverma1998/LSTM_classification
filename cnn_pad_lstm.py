# -*- coding: utf-8 -*-
"""CNN_pad_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uESVJFzrhW0RcNA3KeAmBwP06qP9I5Ga
"""

import numpy as np
!pip install keras_sequential_ascii
import keras
import pandas as pd
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Reshape
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers import Dense , Flatten ,Embedding,Masking,Input, CuDNNLSTM
from keras.layers import Activation
from sklearn.preprocessing import MinMaxScaler
from keras import backend as K
from keras.layers import LSTM
from keras_sequential_ascii import sequential_model_to_ascii_printout
from keras.utils import plot_model
from keras.utils import np_utils
from keras.optimizers import SGD
from keras.constraints import maxnorm
from keras.callbacks import EarlyStopping
from keras.preprocessing.text import one_hot,Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk import word_tokenize,sent_tokenize
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from keras import metrics
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import sys 
from keras.models import Model

# Detector range (used for generation and representations)
ymin = -2
ymax = +2

def generate_event(type):
  if type == 1: # signal
    averageN = 20
    ymean = 1
    ysigma = 3
    pTslope = 10
  elif type == 2: # background
    averageN = 30
    ymean = -1
    ysigma = 3
    pTslope = 12
  else:
    raise Exception
    
  n = np.random.poisson(averageN)
  particles = []
  for i in range(n):
    y = np.random.normal(ymean, ysigma)
    if not ymin < y < ymax:
      continue
    pT = np.random.exponential(pTslope)
    phi = np.random.uniform(0, 2*np.pi)
    particles.append((pT, y, phi))
  
  return particles

# representing event as list of first N particles

def representation1(event, N, sort=0):
  # N - how many particles to keep
  # sort - by which variable to sort (0=Pt, 1=y, 2=phi)
  sorted_event = sorted(event, key=lambda x: x[sort])
  extended = sorted_event + N*[(-100, -100, -100)] # extending in case event is shorter than N
  shortened = extended[:N]
  flattened = [item for sublist in shortened for item in sublist]
  return flattened

# representing event as list of particles

def representation1_var(event, sort=0):
  # N - how many particles to keep
  # sort - by which variable to sort (0=Pt, 1=y, 2=phi)
  sorted_event = sorted(event, key=lambda x: x[sort])
  flattened = [item for sublist in sorted_event for item in sublist]
  return flattened

# representing event as list of first N moments

def moment(A, B, Na, Nb):
  if not len(A) == len(B):
    raise Exception
  
  if Na == 0 and Nb == 0:
    return 1.0
  if Na == 1 and Nb == 0:
    return np.mean(A)
  if Na == 0 and Nb == 1:
    return np.mean(B)
  
  meanA = np.mean(A)
  meanB = np.mean(B)
  s = 0.0
  for a, b in zip(A, B):
    s += (a - meanA)**Na * (b-meanB)**Nb  
  return s/len(A)


def representation2(event, N):
  # N - number of moments
  data = zip(*event)
  M = []
  calculated_moments = []
  for nA in range(N+1):
    for nB in range(N+1):
      if nA == 0 and nB == 0:
        continue
      for iA in range(len(data)):
        for iB in range(len(data)):
          if nA == 0:
            mm = iB + 10*nB
          elif nB == 0:
            mm = iA + 10*nA
          else:
            mm = iA + 10*iB + 100*nA + 10000*nB
          if mm in calculated_moments:
            continue
          calculated_moments.append(mm)
          A = data[iA]
          B = data[iB]
          m = moment(A, B, nA, nB)
          M.append(m)
  return M

# representing event as a 2D histogram

def representation3(event, Ny, Nphi):  
  # Ny, Nphi - number of bins
  pT, y, phi = zip(*event)
  yrange = (ymin, ymax)
  phirange = (0, 2*np.pi)
  r = (yrange, phirange)
  b = (Ny, Nphi)
  h = np.histogram2d(y, phi, bins=b, range=r, weights=pT)
  return list(h[0].flatten())

# Parameters chosen so that the lengths of vectors in all representation are equal
a = generate_event(2)
b = generate_event(1)
r1 = representation1(a, 14)
r1_var = representation1_var(a)
r2 = representation2(a, 2)
r3 = representation3(a, 10, 10)
r3b = representation3(b, 10, 10)
print (len(a),a)
print (len(r1), r1)
print (len(r1_var), r1_var)
print (len(r2), r2)
print (len(r3), r3)
import matplotlib.cm as cm
plt.imshow(np.reshape(r3,(10,10)), cmap=cm.binary)
plt.show()

plt.imshow(np.reshape(r3b,(10,10)), cmap=cm.binary)
plt.show()



# Dataset size
num_events = 10000

signal_class = []
background_class = []
for i in range(num_events):
  signal_class.append(1)      #1--> Signal, 0--> Background
  background_class.append(0)

r1_signal = []
r1_var_signal = []
r3_signal = []               #Taking all representations and generating events
r1_background =[]
r1_var_background = []
r3_background =[]
for i in range(num_events):
  a = []
  while not a:
    a = generate_event(1)
  r1 = representation1(a, 14)
  r1_var = representation1_var(a)
  r3 = representation3(a, 7, 6)
  r1_signal.append(r1)
  r1_var_signal.append(r1_var)
  r3_signal.append(r3)
  b = []
  while not b:
    b = generate_event(2)
  r1a = representation1(b, 14)
  r1a_var = representation1_var(b)
  r3a = representation3(b, 7, 6)
  r1_background.append(r1a)
  r1_var_background.append(r1a_var)
  r3_background.append(r3a)

R1_sig = np.array(r1_signal)
R1_backg = np.array(r1_background)
R1_var_sig = np.array(r1_var_signal)
R1_var_backg = np.array(r1_var_background)
R3_sig = np.array(r3_signal)
R3_backg = np.array(r3_background)
sig_class = np.array(signal_class)
back_class = np.array(background_class)
len(R1_var_sig[1])



r1 = np.reshape(r1_var_signal[1],[1,len(r1_var_signal[1])])
r1.shape

R3 = np.concatenate((R1_var_sig,R1_var_backg)) #Variable signal data all together

data = np.empty([10000,200,1])
data_class = np.empty([10000,1])

def rand_interval(a = 0., b = 100):
   rnd = a + (b-a)*np.random.random()
   return int(rnd)

Stacked_data = np.concatenate((R1_var_sig,R1_var_backg))
                                                          # stacking the Signal and BAckground events  for classification
Stacked_class = np.concatenate((sig_class,back_class))

from sklearn.utils import shuffle     #shuffling the data
Data_org, Class_org = shuffle(Stacked_data, Stacked_class)

print(Data_org.shape,Class_org.shape)

Data_org

pad_data = pad_sequences(Data_org, padding='post')
print(pad_data.shape)

"""**Pythia dataset creation**"""

with open("drive/My Drive/Signal.txt") as signal:
    sig = [line.split() for line in signal]
with open("drive/My Drive/Background.txt") as background:
    backg = [line.split() for line in background]

sig_arr = np.array(sig)
back_arr = np.array(backg)

from google.colab import drive
drive.mount('/content/drive')

len(sig_arr[1])

for i in range(len(sig_arr)):
  for j in range(len(sig_arr[i])):
    sig_arr[i][j] = float(sig_arr[i][j])

for i in range(len(back_arr)):
  for j in range(len(back_arr[i])):
    back_arr[i][j] = float(back_arr[i][j])

sig_arr[3]

import matplotlib.cm as cm
plt.imshow(np.reshape(sig_arr[3],(5,5)), cmap=cm.binary)
plt.show()

sig_arr.shape[0]

signal_pythia_class = []
background_pythia_class = []
for i in range(sig_arr.shape[0]):
  signal_pythia_class.append(1)      #1--> Signal, 0--> Background
  background_pythia_class.append(0)

sig_pythia_class = np.array(signal_pythia_class)
back_pythia_class = np.array(background_pythia_class)

Stacked_data_pythia = np.concatenate((sig_arr,back_arr))
                                                          # stacking the Signal and BAckground events  for classification
Stacked_class_pythia = np.concatenate((sig_pythia_class,back_pythia_class))

from sklearn.utils import shuffle     #shuffling the data
Data_org_pythia, Class_org_pythia = shuffle(Stacked_data_pythia, Stacked_class_pythia)

print(Data_org_pythia.shape,Class_org_pythia.shape)

pad_data_pythia = pad_sequences(Data_org_pythia, padding='post')
print(pad_data_pythia.shape)

import matplotlib.cm as cm
plt.imshow(np.reshape(pad_data_pythia[99],(13,16)), cmap=cm.binary)
plt.show()

"""NEW DATA ANALYSIS"""

with open("drive/My Drive/Difr/Difr00.txt") as new_sig:
    new_signal = [line.split() for line in new_sig]
with open("drive/My Drive/MinBias/MinBias00.txt") as new_backg:
    new_background = [line.split() for line in new_backg]



new_sig_arr = np.array(new_signal)
new_back_arr = np.array(new_background)

for i in range(len(new_sig_arr)):
  for j in range(len(new_sig_arr[i])):
    new_sig_arr[i][j] = float(new_sig_arr[i][j])

for i in range(len(new_back_arr)):
  for j in range(len(new_back_arr[i])):
    new_back_arr[i][j] = float(new_back_arr[i][j])

new_sig_arr.shape

new_signal_pythia_class = []
new_background_pythia_class = []
for i in range(new_sig_arr.shape[0]):
  new_signal_pythia_class.append(1)      #1--> Signal, 0--> Background
  new_background_pythia_class.append(0)

new_sig_pythia_class = np.array(new_signal_pythia_class)
new_back_pythia_class = np.array(new_background_pythia_class)

new_Stacked_data_pythia = np.concatenate((new_sig_arr,new_back_arr))
                                                          # stacking the Signal and BAckground events  for classification
new_Stacked_class_pythia = np.concatenate((new_sig_pythia_class,new_back_pythia_class))

from sklearn.utils import shuffle     #shuffling the data
new_Data_org_pythia, new_Class_org_pythia = shuffle(new_Stacked_data_pythia, new_Stacked_class_pythia)

Data_copy = np.copy(new_Data_org_pythia)

"""Pt Sorting"""

new_Data_org_pythia[1]

for k in range(len(Data_copy)):
  s = 0
  for i in range(2,len(Data_copy[k]),3):
    #print( k,"space", i)
    del new_Data_org_pythia[k][i-s]
    s = s+1

Data_copy[1]

for k in range(len(new_Data_org_pythia)):
 
  for i in range(1,len(new_Data_org_pythia[k]),3): 

    min_idx = i 
    for j in range(i+3,len(new_Data_org_pythia[k]),3): 
        if new_Data_org_pythia[k][min_idx] > new_Data_org_pythia[k][j]: 
            min_idx = j 
         
    new_Data_org_pythia[k][i], new_Data_org_pythia[k][min_idx] = new_Data_org_pythia[k][min_idx], new_Data_org_pythia[k][i] 
    new_Data_org_pythia[k][i+1], new_Data_org_pythia[k][min_idx+1] = new_Data_org_pythia[k][min_idx+1], new_Data_org_pythia[k][i+1] 
    new_Data_org_pythia[k][i-1], new_Data_org_pythia[k][min_idx-1] = new_Data_org_pythia[k][min_idx-1], new_Data_org_pythia[k][i-1]
    
#print ("Sorted array") 
#for i in range(len(new_Data_org_pythia[4])): 
    #print("%f" %new_Data_org_pythia[4][i]),

new_Data_org_pythia[4]

new_pad_data_pythia = pad_sequences(new_Data_org_pythia, padding='post')
print(new_pad_data_pythia.shape)

"""**EMbedding layer idea**"""

input=Input(shape=(pad_data.shape[0],pad_data.shape[1]),dtype='float64')

word_input=Input(shape=(pad_data.shape[1],),dtype='float64')

word_embedding=Embedding(input_dim=150,output_dim=1,input_length=pad_data.shape[1])(word_input)
word_vec=Flatten()(word_embedding)
embed_model =Model([word_input],word_vec)

embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc'])

print(type(word_embedding))
print(word_embedding)

pad_data = abs(pad_data)

print(embed_model.summary())

embed=embed_model.predict(pad_data)

print("Shape of embeddings : ",embed.shape)
print(embed)

"""**Ends Here**"""

pad_data[1:50000].shape

X_train, X_test, y_train, y_test = train_test_split( new_pad_data_pythia[0:80000], new_Class_org_pythia[0:80000], test_size=0.3, random_state=42)  #Train-test spliting
X_test.shape

X_val = X_test[12000:24000]
y_val = y_test[12000:24000]
X_test = X_test[0:12000]
y_test = y_test[0:12000]

print( X_train.shape,  X_test.shape, y_train.shape, y_test.shape)

y_train = np_utils.to_categorical(y_train, 2)
y_test = np_utils.to_categorical(y_test, 2)                   #Conversion of 1 and 0 to binary units for categorical cross entropy
y_val = np_utils.to_categorical(y_val, 2)

X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))
X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))
X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))

X_train.shape

# Model variables
batch_size = 100
epochs = 5

# Used datasets
data_train = X_train
data_test = X_test 
data_val = X_val
class_train = y_train
class_test = y_test
class_val = y_val

def gen_train(X,y):
  while True:
    for i in range(len(X_train)):
      x_n = np.expand_dims(X_train[i],1)
      x_n = np.reshape(x_n,(1,x_n.shape[0],1))
      y_n = np.reshape(y[i],(1,y.shape[1]))
      yield x_n,y_n

def gen_test(X,y):
  while True:
    for i in range(len(X_test)):
      x_n = np.expand_dims(X_train[i],1)
      x_n = np.reshape(x_n,(1,x_n.shape[0],1))
      y_n = np.reshape(y[i],(1,y.shape[1]))
      yield x_n,y_n

def base_model():
  model = Sequential()
  #model.add(Embedding(10000, 1))
  model.add(Masking(mask_value=0, input_shape=(None,1)))
  #model.add(Flatten())
  model.add(LSTM(35,input_shape=(None,1),return_sequences=False))
  #model.add(LSTM(10,input_shape=(None,1),return_sequences=False))
  #model.add(LSTM(25,return_sequences=False))
  #model.add(Dense(100,activation='relu',input_shape=(42,),kernel_constraint=maxnorm(3)))
  #model.add(Dropout(rate = 0.2))
  #model.add(LSTM(10,input_shape=(42,1),return_sequences=False))
  #model.add(Dense(50,activation='relu',kernel_constraint=maxnorm(3)))
  #model.add(Dropout(rate = 0.3))
  #model.add(Dense(20,activation='relu',kernel_constraint=maxnorm(3)))                #Dense feed forward Neural Network(4-layers) model
  #model.add(Dropout(rate = 0.3))
  #model.add(Dense(5,activation='relu',kernel_constraint=maxnorm(3)))
  model.add(Dropout(rate = 0.3))
  
  model.add(Dense(2, activation='softmax'))
  ##model.add(Reshape((1,2),input_shape=(2,)))
  
  #model.add(Reshape((1,2),input_shape=(2,)))
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.categorical_accuracy]) 
  return model

event_cf = base_model()
event_cf.summary()
#sequential_model_to_ascii_printout(event_cf)
plot_model(event_cf)

es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=10)
mc = ModelCheckpoint('best_model_lstm_mask.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)
event_cf = event_cf.fit(data_train, class_train, batch_size=batch_size, epochs=epochs, validation_data=(data_val,class_val),shuffle=True, callbacks=[es, mc])
saved_model = load_model('best_model_lstm_mask.h5')

# evaluate loaded model
scores_train = saved_model.evaluate(data_train, class_train, verbose=0)
scores_test  = saved_model.evaluate(data_test,  class_test,  verbose=0)
scores_val   = saved_model.evaluate(data_val,   class_val,   verbose=0)
print("Accuracy Train: %.2f%% , Test: %.2f%% Val: %.2f%% " % (scores_train[1]*100, scores_test[1]*100, scores_val[1]*100))         #Efficiency(train and test)

YY_pred = saved_model.predict(data_test, verbose=2)

YY_pred = YY_pred > 0.5

YY_pred

class_test = (class_test>0.5)
class_test

import matplotlib.pyplot as plt
n_epochs = len(event_cf.history['loss'])

plt.figure(0)
plt.plot(event_cf.history['categorical_accuracy'],'r')
plt.plot(event_cf.history['val_categorical_accuracy'],'g')
plt.xticks(np.arange(0, 30, 5))
plt.rcParams['figure.figsize'] = (10, 6)
plt.xlabel("Num of Epochs")                                                   #plotting of test/val error and training error
plt.ylabel("Accuracy")
plt.title("Training Cat_Accuracy vs Validation Cat_Accuracy")
plt.legend(['train','validation'])

plt.figure(1)
plt.plot(event_cf.history['loss'],'r')
plt.plot(event_cf.history['val_loss'],'g')
plt.xticks(np.arange(0, 30, 5))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Loss")
plt.title("Training Loss vs Validation Loss")
plt.legend(['train','validation'])
plt.show()

# Confusion matrix result

from sklearn.metrics import classification_report, confusion_matrix
YY_pred = saved_model.predict(data_test, verbose=2)
yy_pred = np.argmax(YY_pred, axis=1)

yy_test2 = np.argmax(class_test, axis=1)



#confusion matrix
cm = confusion_matrix(np.argmax(class_test,axis=1),yy_pred)                         #Confusion matrix
print(cm)

# Visualizing of confusion matrix
import seaborn as sn
import pandas  as pd


df_cm = pd.DataFrame(cm, range(2),
                  range(2))
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size
plt.show()



"""**HISTOGRAM ANALYSIS, old data**"""

print(new_pad_data_pythia.shape,new_Class_org_pythia.shape)

hist_train, hist_test, label_train, label_test = train_test_split( new_pad_data_pythia, new_Class_org_pythia, test_size=0.3, random_state=42)  #Train-test spliting
hist_test.shape

hist_val = hist_test[30000:60000]
label_val = label_test[30000:60000]
hist_test = hist_test[0:30000]
label_test = label_test[0:30000]

x = hist_train[1]
y = hist_train[1]
len(hist_train)



hist_train.shape

len(hist_train[10][1:129])

import matplotlib.cm as cm
plt.title(label_test[201])
plt.imshow(np.reshape(hist_test[201],(6,15)), cmap=cm.binary)
plt.show()

label_train = np_utils.to_categorical(label_train, 2)
label_test = np_utils.to_categorical(label_test, 2)
label_val = np_utils.to_categorical(label_val, 2)
print(label_test.shape)

img_rows = 6
img_cols = 15
img_size_flat = img_cols*img_rows
num_channels = 1
num_classes =2

hist_train = np.reshape(hist_train,(hist_train.shape[0],1,img_rows,img_cols))
hist_test = np.reshape(hist_test,(hist_test.shape[0],1,img_rows,img_cols))
hist_val = np.reshape(hist_test,(hist_val.shape[0],1,img_rows,img_cols))
print(hist_train.shape,hist_test.shape,hist_val.shape)

# Define Model

def base_model():

    model = Sequential()
    input_shape=(1,img_rows,img_cols)

    model.add(Reshape((img_rows,img_cols,1),input_shape=input_shape))
    model.add(Conv2D(64,kernel_size = 2,activation='relu',input_shape=input_shape))
    #model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Conv2D(50,kernel_size = 2,activation='relu',input_shape=input_shape))
    #model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dropout(rate = 0.2))
    model.add(Dense(320,activation='relu',kernel_constraint=maxnorm(3))) #1024
    model.add(Dropout(rate = 0.3))
    model.add(Dense(160,activation='relu',kernel_constraint=maxnorm(3))) #1024
    model.add(Dropout(rate = 0.3))
    #model.add(Dense(60,activation='relu',kernel_constraint=maxnorm(3))) #1024
    #model.add(Dropout(rate = 0.3))

    model.add(Dense(num_classes, activation='softmax'))

    #model.add(Reshape((1,num_classes),input_shape=(num_classes,)))

#    sgd = SGD(lr = 0.1, decay=1e-6, nesterov=True)
###    sgd = SGD(lr = 0.01, decay=1e-6, nesterov=True)

# Train model

#    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[metrics.categorical_accuracy])   # 'accuracy'
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.categorical_accuracy]) 
    return model

# Vizualizing model structure
# Model summary (ascii)
cnn_n = base_model()
cnn_n.summary()

# Model printout (ascii)
sequential_model_to_ascii_printout(cnn_n)

# Plotting model in graphical mode
plot_model(cnn_n, to_file='6th_CNN.png')  ###, show_shapes=True

# patient early stopping
es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=10)
mc = ModelCheckpoint('best_model_hist_conv2d.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)
# fit model
cnn = cnn_n.fit(hist_train, label_train, batch_size=100, epochs=5, validation_data=(hist_test,label_test),shuffle=True, callbacks=[es, mc])
# load the saved model
saved_model = load_model('best_model_hist_conv2d.h5')

# evaluate loaded model
scores_train = saved_model.evaluate(hist_train, label_train, verbose=0)
scores_test  = saved_model.evaluate(hist_test,  label_test,  verbose=0)
scores_val   = saved_model.evaluate(hist_val,   label_val,   verbose=0)
print("Accuracy Train: %.2f%% , Test: %.2f%% Val: %.2f%% " % (scores_train[1]*100, scores_test[1]*100, scores_test[1]*100))          #Efficiency(train and test)

# Plots for training and testing process: loss and accuracy

n_epochs = len(cnn.history['loss'])

plt.figure(0)
plt.plot(cnn.history['categorical_accuracy'],'r')
plt.plot(cnn.history['val_categorical_accuracy'],'g')
plt.xticks(np.arange(0, n_epochs, 10.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Accuracy")
plt.title("Training Cat_Accuracy vs Test Cat_Accuracy")
plt.legend(['train','validation'])

plt.figure(1)
plt.plot(cnn.history['loss'],'r')
plt.plot(cnn.history['val_loss'],'g')
plt.xticks(np.arange(0, n_epochs, 10.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Loss")
plt.title("Training Loss vs Test Loss")
plt.legend(['train','Test'])
plt.show()

# Confusion matrix result

from sklearn.metrics import classification_report, confusion_matrix
YY_pred = saved_model.predict(hist_test, verbose=2)
yy_pred = np.argmax(YY_pred, axis=1)

yy_test2 = np.argmax(label_test, axis=1)



#confusion matrix
cm = confusion_matrix(np.argmax(label_test,axis=1),yy_pred)                         #Confusion matrix
print(cm)

# Visualizing of confusion matrix
import seaborn as sn
import pandas  as pd


df_cm = pd.DataFrame(cm, range(2),
                  range(2))
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size
plt.show()



"""**THREE CLASS CLASSIFICATION....( SD , DD AND BACKGROUND)**"""

with open("drive/My Drive/DblDifr/DblDifr00.txt") as dbl_difr:
    dbl_dif = [line.split() for line in dbl_difr]
with open("drive/My Drive/SinDifr/SinDifr00.txt") as sin_difr:
    sin_dif = [line.split() for line in sin_difr]
with open("drive/My Drive/MinBias/MinBias00.txt") as bckg:
    difr_bckg = [line.split() for line in bckg]

dbl_diff = np.array(dbl_dif)
sin_diff = np.array(sin_dif)
diff_bckg = np.array(difr_bckg)

for i in range(len(dbl_diff)):
  for j in range(len(dbl_diff[i])):
    dbl_diff[i][j] = float(dbl_diff[i][j])

for i in range(len(sin_diff)):
  for j in range(len(sin_diff[i])):
    sin_diff[i][j] = float(sin_diff[i][j])

for i in range(len(diff_bckg)):
  for j in range(len(diff_bckg[i])):
    diff_bckg[i][j] = float(diff_bckg[i][j])

dbl_diff_class = []
sin_diff_class = []
for i in range(dbl_diff.shape[0]):
  dbl_diff_class.append(1)      
  sin_diff_class.append(0)

diff_bckg_class = []
for i in range(diff_bckg.shape[0]):
  diff_bckg_class.append(2)

sin_dif_class = np.array(sin_diff_class)
dbl_dif_class = np.array(dbl_diff_class)
dif_bckg_class = np.array(diff_bckg_class)

diff_Stacked_data = np.concatenate((sin_diff,dbl_diff))
diff_Stacked_data = np.concatenate((diff_Stacked_data,diff_bckg))
                                                          # stacking the Signal and BAckground events  for classification
diff_Stacked_class = np.concatenate((sin_diff_class,dbl_diff_class))
diff_Stacked_class = np.concatenate((diff_Stacked_class,diff_bckg_class))

diff_Stacked_class.shape

from sklearn.utils import shuffle     #shuffling the data
diff_Data_org, diff_Class_org = shuffle(diff_Stacked_data, diff_Stacked_class)

Data_copy = np.copy(diff_Data_org)

for k in range(len(Data_copy)):
  s = 0
  for i in range(2,len(Data_copy[k]),3):
    #print( k,"space", i)
    del diff_Data_org[k][i-s]
    s = s+1

'''
for k in range(len(diff_Data_org)):
 
  for i in range(1,len(diff_Data_org[k]),3): 

    min_idx = i 
    for j in range(i+3,len(diff_Data_org[k]),3): 
        if diff_Data_org[k][min_idx] > diff_Data_org[k][j]: 
            min_idx = j 
         
    diff_Data_org[k][i], diff_Data_org[k][min_idx] = diff_Data_org[k][min_idx], diff_Data_org[k][i] 
    diff_Data_org[k][i+1], diff_Data_org[k][min_idx+1] = diff_Data_org[k][min_idx+1], diff_Data_org[k][i+1] 
    diff_Data_org[k][i-1], diff_Data_org[k][min_idx-1] = diff_Data_org[k][min_idx-1], diff_Data_org[k][i-1]
   
#print ("Sorted array") 
#for i in range(len(new_Data_org_pythia[4])): 
    #print("%f" %new_Data_org_pythia[4][i]),  
    '''

diff_pad_data = pad_sequences(diff_Data_org, padding='post')
print(diff_pad_data.shape)

diff_train, diff_test, diff_label_train, diff_label_test = train_test_split( diff_pad_data[0:80000], diff_Class_org[0:80000], test_size=0.3, random_state=42)  #Train-test spliting
diff_test.shape

diff_val = diff_test[12000:24000]
diff_label_val = diff_label_test[12000:24000]
diff_test = diff_test[0:12000]
diff_label_test = diff_label_test[0:12000]

print( diff_train.shape,  diff_test.shape, diff_label_train.shape, diff_label_test.shape)

diff_label_train = np_utils.to_categorical(diff_label_train, 3)
diff_label_test = np_utils.to_categorical(diff_label_test, 3)                   #Conversion of 1 and 0 to binary units for categorical cross entropy
diff_label_val = np_utils.to_categorical(diff_label_val, 3)

diff_train = np.reshape(diff_train,(diff_train.shape[0],diff_train.shape[1],1))
diff_test = np.reshape(diff_test,(diff_test.shape[0],diff_test.shape[1],1))
diff_val = np.reshape(diff_val,(diff_val.shape[0],diff_val.shape[1],1))

# Model variables
batch_size = 100
epochs = 5

# Used datasets
diff_data_train = diff_train
diff_data_test = diff_test 
diff_data_val = diff_val
diff_class_train = diff_label_train
diff_class_test = diff_label_test
diff_class_val = diff_label_val

def base_model():
  model = Sequential()
  #model.add(Embedding(10000, 1))
  model.add(Masking(mask_value=0, input_shape=(None,1)))
  #model.add(Flatten())
  model.add(LSTM(35,input_shape=(None,1),return_sequences=False))
  #model.add(LSTM(10,input_shape=(None,1),return_sequences=False))
  #model.add(LSTM(25,return_sequences=False))
  #model.add(Dense(100,activation='relu',input_shape=(42,),kernel_constraint=maxnorm(3)))
  #model.add(Dropout(rate = 0.2))
  #model.add(LSTM(10,input_shape=(42,1),return_sequences=False))
  #model.add(Dense(50,activation='relu',kernel_constraint=maxnorm(3)))
  #model.add(Dropout(rate = 0.3))
  #model.add(Dense(20,activation='relu',kernel_constraint=maxnorm(3)))                #Dense feed forward Neural Network(4-layers) model
  #model.add(Dropout(rate = 0.3))
  #model.add(Dense(5,activation='relu',kernel_constraint=maxnorm(3)))
  model.add(Dropout(rate = 0.3))
  
  model.add(Dense(3, activation='softmax'))
  ##model.add(Reshape((1,2),input_shape=(2,)))
  
  #model.add(Reshape((1,2),input_shape=(2,)))
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.categorical_accuracy]) 
  return model

diff_event_cf = base_model()
diff_event_cf.summary()
#sequential_model_to_ascii_printout(event_cf)
plot_model(diff_event_cf)

es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=10)
mc = ModelCheckpoint('best_model_three_lstm_mask.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)
diff_event_cf = diff_event_cf.fit(diff_data_train, diff_class_train, batch_size=batch_size, epochs=epochs, validation_data=(diff_data_val,diff_class_val),shuffle=True, callbacks=[es, mc])
saved_model = load_model('best_model_three_lstm_mask.h5')

# evaluate loaded model
scores_train = saved_model.evaluate(diff_data_train, diff_class_train, verbose=0)
scores_test  = saved_model.evaluate(diff_data_test,  diff_class_test,  verbose=0)
scores_val   = saved_model.evaluate(diff_data_val,   diff_class_val,   verbose=0)
print("Accuracy Train: %.2f%% , Test: %.2f%% Val: %.2f%% " % (scores_train[1]*100, scores_test[1]*100, scores_val[1]*100))          #Efficiency(train and test)



# Confusion matrix result

from sklearn.metrics import classification_report, confusion_matrix
YY_pred = saved_model.predict(diff_data_test, verbose=2)
yy_pred = np.argmax(YY_pred, axis=1)

yy_test2 = np.argmax(diff_class_test, axis=1)



#confusion matrix
cm = confusion_matrix(np.argmax(diff_class_test,axis=1),yy_pred)                         #Confusion matrix
print(cm)

# Visualizing of confusion matrix
import seaborn as sn
import pandas  as pd


df_cm = pd.DataFrame(cm, range(3),
                  range(3))
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size
plt.show()



diff_hist_train, diff_hist_test, diff_label_train, diff_label_test = train_test_split( diff_pad_data, diff_Class_org, test_size=0.3, random_state=42)  #Train-test spliting
diff_hist_train.shape

diff_hist_val = diff_hist_test[15000:30000]
diff_label_val = diff_label_test[15000:30000]
diff_hist_test = diff_hist_test[0:15000]
diff_label_test = diff_label_test[0:15000]

diff_label_test

import matplotlib.cm as cm
plt.title(diff_label_test[-3])
plt.imshow(np.reshape(diff_hist_test[-3],(6,15)), cmap=cm.binary)
plt.show()

diff_label_train = np_utils.to_categorical(diff_label_train, 3)
diff_label_test = np_utils.to_categorical(diff_label_test, 3)
diff_label_val = np_utils.to_categorical(diff_label_val, 3)
print(diff_label_test.shape)

img_rows =15
img_cols =6
img_size_flat = img_cols*img_rows
num_channels = 1
num_classes =3

diff_hist_train = np.reshape(diff_hist_train,(diff_hist_train.shape[0],1,img_rows,img_cols))
diff_hist_test = np.reshape(diff_hist_test,(diff_hist_test.shape[0],1,img_rows,img_cols))
diff_hist_val = np.reshape(diff_hist_test,(diff_hist_val.shape[0],1,img_rows,img_cols))
print(diff_hist_train.shape,diff_hist_test.shape,diff_hist_val.shape)

# Define Model

def base_model():

    model = Sequential()
    input_shape=(1,img_rows,img_cols)

    model.add(Reshape((img_rows,img_cols,1),input_shape=input_shape))
    model.add(Conv2D(80,kernel_size = 2,activation='relu',input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(ZeroPadding2D(5))
    #model.add(Conv2D(40,kernel_size = 2,activation='relu',input_shape=input_shape))
    #model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Conv2D(64,kernel_size = 3,activation='relu',input_shape=input_shape))
    #model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dropout(rate = 0.2))
    model.add(Dense(32*img_rows,activation='relu',kernel_constraint=maxnorm(3))) #1024
    model.add(Dropout(rate = 0.3))
    #model.add(Dense(32*img_rows,activation='relu',kernel_constraint=maxnorm(3))) #1024
    #model.add(Dropout(rate = 0.3))


    model.add(Dense(num_classes, activation='softmax'))

    #model.add(Reshape((1,num_classes),input_shape=(num_classes,)))

#    sgd = SGD(lr = 0.1, decay=1e-6, nesterov=True)
###    sgd = SGD(lr = 0.01, decay=1e-6, nesterov=True)

# Train model

#    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[metrics.categorical_accuracy])   # 'accuracy'
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.categorical_accuracy]) 
    return model

# Vizualizing model structure
# Model summary (ascii)
diff_cnn_n = base_model()
diff_cnn_n.summary()

# Model printout (ascii)
sequential_model_to_ascii_printout(diff_cnn_n)

# Plotting model in graphical mode
plot_model(diff_cnn_n)   ###, to_file='model.png')  ###, show_shapes=True

# patient early stopping
es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=10)
mc = ModelCheckpoint('best_model_diff_hist_conv2d.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)
# fit model
diff_cnn = diff_cnn_n.fit(diff_hist_train, diff_label_train, batch_size=100, epochs=5, validation_data=(diff_hist_test,diff_label_test),shuffle=True, callbacks=[es, mc])
# load the saved model
saved_model = load_model('best_model_diff_hist_conv2d.h5')

# evaluate loaded model
scores_train = saved_model.evaluate(diff_hist_train, diff_label_train, verbose=0)
scores_test  = saved_model.evaluate(diff_hist_test,  diff_label_test,  verbose=0)
scores_val   = saved_model.evaluate(diff_hist_val,   diff_label_val,   verbose=0)
print("Accuracy Train: %.2f%% , Test: %.2f%% Val: %.2f%% " % (scores_train[1]*100, scores_test[1]*100, scores_test[1]*100))          #Efficiency(train and test)

# Plots for training and testing process: loss and accuracy

n_epochs = len(diff_cnn.history['loss'])

plt.figure(0)
plt.plot(cnn.history['categorical_accuracy'],'r')
plt.plot(cnn.history['val_categorical_accuracy'],'g')
plt.xticks(np.arange(0, n_epochs, 10.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Accuracy")
plt.title("Training Cat_Accuracy vs Test Cat_Accuracy")
plt.legend(['train','validation'])

plt.figure(1)
plt.plot(cnn.history['loss'],'r')
plt.plot(cnn.history['val_loss'],'g')
plt.xticks(np.arange(0, n_epochs, 10.0))
plt.rcParams['figure.figsize'] = (8, 6)
plt.xlabel("Num of Epochs")
plt.ylabel("Loss")
plt.title("Training Loss vs Test Loss")
plt.legend(['train','Test'])
plt.show()

# Confusion matrix result

from sklearn.metrics import classification_report, confusion_matrix
YY_pred = saved_model.predict(diff_hist_test, verbose=2)
yy_pred = np.argmax(YY_pred, axis=1)

yy_test2 = np.argmax(diff_label_test, axis=1)



#confusion matrix
cm = confusion_matrix(np.argmax(diff_label_test,axis=1),yy_pred)                         #Confusion matrix
print(cm)

# Visualizing of confusion matrix
import seaborn as sn
import pandas  as pd


df_cm = pd.DataFrame(cm, range(3),
                  range(3))
plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)#for label size
sn.heatmap(df_cm, annot=True,annot_kws={"size": 12})# font size
plt.show()

types = ["w/o sort", "Pt sort","eta sort"]
types_conv = ["Min_diff","Max_dist"]

ACC_CNN_2_Train = [91.12,91.15,91.07,59.64]
ACC_CNN_3_Train = [63.8,63.365,63.11,40.49]

ACC_CNN_2_Test = [90.9,91.2,91.225]
ACC_CNN_3_Test = [63.36,63.5,63.5]

ACC_CNN_2_Val = [90.9,91.2,91.225,59.4]
ACC_CNN_3_Val = [63.26,63.6,63.41,41.2]

ACC_LSTM_2_Train = [84.0175,84.9,84.62,59.72]
ACC_LSTM_2_var_Train = [95.1,94.0,94.9,65.0]
ACC_LSTM_2_var_Test = [94.9,93.6,94.8]
ACC_LSTM_2_var_Val = [94.4,93.5,94.6,64.3]

ACC_LSTM_2_Test = [84.02,84.46,85.02]
ACC_LSTM_3_Test = [57.93,58.71,58.3]

ACC_LSTM_2_Val = [84.1,84.92,84.6,59.712]
ACC_LSTM_3_Val = [58.12,58.227,58.3,41.16]

ACC_2_CONV_Train =[64.2,63.5,63.8]
ACC_2_CONV_Test =[64.3,63.6,63.8 ]
ACC_2_CONV_val =[ 64.1,63.4]
ACC_3_CONV_Train =[45.8,46.1 ]
ACC_3_CONV_Test =[45.9,46.5,46.7 ]
ACC_3_CONV_val =[ 45.6,45.9]

ACC_LSTM_3_Train = [58.31,58.35,58.335,40.74]
ACC_LSTM_3_var_Train = [68.7,65.4,66.9]
ACC_LSTM_3_var_Test = [65.7,64.9,66.9]
ACC_LSTM_3_var_Val = [64.7,64.7,64.5,46.9]



plt.figure(figsize=(12,8))
#plt.plot(types_conv,ACC_2_CONV_Train,label="Conv_train_double")
plt.plot(types,ACC_2_CONV_Test,label="Conv_test_double")
#plt.plot(types_conv,ACC_2_CONV_val,label="Conv_val_double")
#plt.plot(types_conv,ACC_3_CONV_Train,label="Conv_train_triple")
plt.plot(types_conv,ACC_3_CONV_Test,label="Conv_test_triple")
#plt.plot(types_conv,ACC_3_CONV_val,label="Conv_val_triple")
ax = plt.gca()
ax.tick_params(width=5,labelsize=20)
plt.legend()
plt.ylabel("Test_Accuracy(%)",fontsize=20)
plt.title("Conventional way(STAR)",fontsize=20)

import matplotlib
plt.figure(figsize=(12,8))
#plt.plot(types,ACC_CNN_3_Train,label="CNN train")
#plt.plot(types,ACC_LSTM_3_Train,label = "LSTM_train")
#plt.plot(types,ACC_LSTM_3_var_Train,label = "LSTM_var_train")
plt.plot(types,ACC_2_CONV_Test,label="Conv_test_double")
plt.plot(types,ACC_3_CONV_Test,label="Conv_test_triple")
plt.plot(types,ACC_CNN_3_Test,label="CNN test_3")
plt.plot(types,ACC_LSTM_3_Test,label = "LSTM_test_3")
plt.plot(types,ACC_LSTM_3_var_Test,label = "LSTM_var_test_3")
plt.plot(types,ACC_CNN_2_Test,label="CNN test_2")
plt.plot(types,ACC_LSTM_2_Test,label = "LSTM_test_2")
plt.plot(types,ACC_LSTM_2_var_Test,label = "LSTM_var_test_2")
#plt.plot(types,ACC_CNN_3_Val,label="CNN val")
#plt.plot(types,ACC_LSTM_3_Val,label = "LSTM_val")
#plt.plot(types,ACC_LSTM_3_var_Val,label = "LSTM_var_val")
plt.text("w/o sort",95,'LSTM-var',size=20)
plt.text("w/o sort",67,'LSTM-var',size=20)
plt.text("w/o sort",59,'LSTM-fixed',size=20)
plt.text("w/o sort",48,'Conv-3_class',size=20)
plt.text("w/o sort",91.6,'CNN',size=20)
plt.text("w/o sort",85,'LSTM-fixed',size=20)
plt.text("Pt sort",85,'2-class',size=30)
plt.text("Pt sort",50,'3-class',size=30)
plt.text("Pt sort",61,'CNN',size=20)
#plt.text("eta sort",61,'Conv-2_Class',size=20)
ax = plt.gca()
ax.tick_params(width=5,labelsize=20)
plt.ylabel("Accuracy(%)",fontsize=20)
plt.title("Classification(STAR)",fontsize=20)

LHC_LSTM_2_fixed_Test = [78.5,78.3,78.2]
LHC_LSTM_2_var_Test = [95.9,96.6,95.5]
LHC_CNN_2_Test = [95.415,95.2,95.32]
LHC_LSTM_3_fixed_Test = [56.25,56.383,56.26]
LHC_LSTM_3_var_Test = [69.6,67.8,67.3]
LHC_CNN_3_Test = [68.14,67.2,67.086]

LHC_Conv_2_Test = [68.2,69.1,69.2]
LHC_Conv_3_Test = [49.1,49.1,49.2]

import matplotlib
plt.figure(figsize=(12,8))
#plt.plot(types,ACC_CNN_3_Train,label="CNN train")
#plt.plot(types,ACC_LSTM_3_Train,label = "LSTM_train")
#plt.plot(types,ACC_LSTM_3_var_Train,label = "LSTM_var_train")
plt.plot(types,LHC_CNN_3_Test,label="CNN test_3")
plt.plot(types,LHC_LSTM_3_fixed_Test,label = "LSTM_test_3")
plt.plot(types,LHC_LSTM_3_var_Test,label = "LSTM_var_test_3")
plt.plot(types,LHC_CNN_2_Test,label="CNN test_2")
plt.plot(types,LHC_LSTM_2_fixed_Test,label = "LSTM_test_2")
plt.plot(types,LHC_LSTM_2_var_Test,label = "LSTM_var_test_2")
plt.plot(types,LHC_Conv_2_Test,label="Conv_test_double")
plt.plot(types,LHC_Conv_3_Test,label="Conv_test_triple")
plt.text("w/o sort",97,'LSTM-var',size=20)
plt.text("w/o sort",71,'LSTM-var',size=20)
plt.text("w/o sort",57,'LSTM-fixed',size=20)
plt.text("w/o sort",49.6,'Conv-3_class',size=20)
plt.text("Pt sort",70,'Conv-2_class',size=20)
plt.text("w/o sort",93.5,'CNN',size=20)
plt.text("w/o sort",79,'LSTM-fixed',size=20)
plt.text("Pt sort",85,'2-class',size=30)
plt.text("Pt sort",50,'3-class',size=30)
plt.text("Pt sort",65,'CNN',size=20)

#plt.plot(types,ACC_CNN_3_Val,label="CNN val")
#plt.plot(types,ACC_LSTM_3_Val,label = "LSTM_val")
#plt.plot(types,ACC_LSTM_3_var_Val,label = "LSTM_var_val")
ax = plt.gca()
ax.tick_params(width=5,labelsize=20)
plt.ylabel("Accuracy(%)",fontsize=20)
plt.title("Classification(LHC)",fontsize=20)





import matplotlib
plt.figure(figsize=(12,8))
#plt.plot(types,ACC_CNN_3_Train,label="CNN train")
#plt.plot(types,ACC_LSTM_3_Train,label = "LSTM_train")
#plt.plot(types,ACC_LSTM_3_var_Train,label = "LSTM_var_train")
#plt.plot(types,ACC_2_CONV_Test,label="Conv_test_double")
plt.plot(types,ACC_3_CONV_Test,label="Conv_test_triple")
plt.plot(types,ACC_CNN_3_Test,label="CNN test_3")
plt.plot(types,ACC_LSTM_3_Test,label = "LSTM_test_3")
plt.plot(types,ACC_LSTM_3_var_Test,label = "LSTM_var_test_3")
#plt.plot(types,ACC_CNN_2_Test,label="CNN test_2")
#plt.plot(types,ACC_LSTM_2_Test,label = "LSTM_test_2")
#plt.plot(types,ACC_LSTM_2_var_Test,label = "LSTM_var_test_2")
#plt.plot(types,ACC_CNN_3_Val,label="CNN val")
#plt.plot(types,ACC_LSTM_3_Val,label = "LSTM_val")
#plt.plot(types,ACC_LSTM_3_var_Val,label = "LSTM_var_val")
#plt.text("w/o sort",95,'LSTM-var',size=20)
plt.text("w/o sort",66,'LSTM-var',size=20)
plt.text("w/o sort",59,'LSTM-fixed',size=20)
plt.text("w/o sort",47,'Conv-3_class',size=20)
#plt.text("w/o sort",91.6,'CNN',size=20)
#plt.text("w/o sort",85,'LSTM-fixed',size=20)
#plt.text("Pt sort",85,'2-class',size=30)
#plt.text("Pt sort",50,'3-class',size=30)
plt.text("Pt sort",62,'CNN',size=20)
#plt.text("eta sort",61,'Conv-2_Class',size=20)
ax = plt.gca()
ax.tick_params(width=5,labelsize=20)
plt.ylabel("Accuracy(%)",fontsize=20)
plt.title("Classification Triple Class(STAR)",fontsize=20)

import matplotlib
plt.figure(figsize=(12,8))
#plt.plot(types,ACC_CNN_3_Train,label="CNN train")
#plt.plot(types,ACC_LSTM_3_Train,label = "LSTM_train")
#plt.plot(types,ACC_LSTM_3_var_Train,label = "LSTM_var_train")
plt.plot(types,LHC_CNN_3_Test,label="CNN test_3")
plt.plot(types,LHC_LSTM_3_fixed_Test,label = "LSTM_test_3")
plt.plot(types,LHC_LSTM_3_var_Test,label = "LSTM_var_test_3")
#plt.plot(types,LHC_CNN_2_Test,label="CNN test_2")
#plt.plot(types,LHC_LSTM_2_fixed_Test,label = "LSTM_test_2")
#plt.plot(types,LHC_LSTM_2_var_Test,label = "LSTM_var_test_2")
#plt.plot(types,LHC_Conv_2_Test,label="Conv_test_double")
plt.plot(types,LHC_Conv_3_Test,label="Conv_test_triple")
#plt.text("w/o sort",96,'LSTM-var',size=20)
plt.text("Pt sort",68,'LSTM-var',size=20)
plt.text("w/o sort",57,'LSTM-fixed',size=20)
plt.text("w/o sort",49.6,'Conv-3_class',size=20)
#plt.text("Pt sort",70,'Conv-2_class',size=20)
#plt.text("w/o sort",93.5,'CNN',size=20)
#plt.text("w/o sort",79,'LSTM-fixed',size=20)
#plt.text("Pt sort",85,'2-class',size=30)
#plt.text("Pt sort",50,'3-class',size=30)
plt.text("Pt sort",66,'CNN',size=20)
#plt.legend()
#plt.plot(types,ACC_CNN_3_Val,label="CNN val")
#plt.plot(types,ACC_LSTM_3_Val,label = "LSTM_val")
#plt.plot(types,ACC_LSTM_3_var_Val,label = "LSTM_var_val")
ax = plt.gca()
ax.tick_params(width=5,labelsize=20)
plt.ylabel("Accuracy(%)",fontsize=20)
plt.title("Classification Triple Class(LHC)",fontsize=20)

